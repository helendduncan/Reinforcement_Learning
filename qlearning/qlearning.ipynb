{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa1eec3",
   "metadata": {},
   "source": [
    "# Using Q-learning on a classic control problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6b417",
   "metadata": {},
   "source": [
    "This notebook will walk you through using Q-learning on a \"classic control\" problem; cart-pole\n",
    "\n",
    "\"The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base.\" - [Sutton, Barto et. al](https://ieeexplore.ieee.org/document/6313077) as implemented by Gymnasium.\n",
    "\n",
    "There are 4 observations: \n",
    "1. The cart position \n",
    "2. The cart velocity\n",
    "3. The pole angle\n",
    "4. The pole angular velocity\n",
    "\n",
    "There are two actions: \n",
    "1. Move the cart left (0)\n",
    "2. Move the cart right (1)\n",
    "\n",
    "There are three ways the task is terminated.\n",
    "1. If the cart moves out of the area of play (±2.4 range)\n",
    "2. If the angle of the pole is greater than ±12 degrees (±0.2095 radian)\n",
    "3. The pole remains upright for more than 500 steps and the task is truncated\n",
    "\n",
    "Q-learning can be applied to finite state Markov decision processes, in this example the observations are continuous, however they can be binned and made discrete as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f2e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np \n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import warnings\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74523695",
   "metadata": {},
   "source": [
    "## One trial\n",
    "\n",
    "First let's run through one trial of cartpole. \n",
    "\n",
    "The observation space has upper and lower limits for the four observations. For the cart position and pole angle they are double what the termination criterium would be. The cart velocity and pole angular velocity have no limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a241465",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176bc71",
   "metadata": {},
   "source": [
    "On reset of the environment, the four observations are set to values in the ±0.05 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b2bcb",
   "metadata": {},
   "source": [
    "The environment is then evolved step-wise, with one action taken at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for loop\n",
    "terminated = False\n",
    "truncated = False\n",
    "score = 0\n",
    "\n",
    "# Make environment\n",
    "env = gym.make('CartPole-v1') #,render_mode=\"human\") # Note you can visualise if you import pygame too\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Plot the results of one game\n",
    "cart_pos = []\n",
    "pole_ang = []\n",
    "# Run for one game\n",
    "while terminated is False and truncated is False:\n",
    "\n",
    "    action = env.action_space.sample()  # Pick either left(0) or right(1)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    cart_pos.append(observation[0])\n",
    "    pole_ang.append(observation[2])\n",
    "    score+=reward\n",
    "\n",
    "env.close()\n",
    "print(f\"The score is {int(score)}\")\n",
    "\n",
    "if terminated:\n",
    "    print(\"The game ended due to one of the two failure conditions being met\")\n",
    "elif truncated:\n",
    "    print(\"The game ended after too many steps\")\n",
    "else:\n",
    "    print(\"The game stopped for some unknown reason\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d4144",
   "metadata": {},
   "source": [
    "Importing pygame will allow the task to be visualised, however for this example where there are two failure conditions, a plot of the cart position and pole angle can show the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ba748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(cart_pos, pole_ang):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-0.3, 0.3])\n",
    "    \n",
    "    xylim=[2.4,0.2095]\n",
    "    ax.vlines(x=xylim[0], ymin=-xylim[1], ymax=xylim[1], linewidth=1, color=\"r\")\n",
    "    ax.vlines(x=-xylim[0], ymin=-xylim[1], ymax=xylim[1], linewidth=1, color=\"r\")\n",
    "    ax.hlines(y=xylim[1], xmin=-xylim[0], xmax=xylim[0], linewidth=1, color='r')\n",
    "    ax.hlines(y=-xylim[1], xmin=-xylim[0], xmax=xylim[0], linewidth=1, color='r')\n",
    "\n",
    "    ax.set_xlabel(\"Cart position\")\n",
    "    ax.set_ylabel(\"Pole Angle (radians)\")\n",
    "\n",
    "    ax.scatter(cart_pos,pole_ang, marker=\"x\",color=\"k\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(cart_pos,pole_ang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b82a4",
   "metadata": {},
   "source": [
    "In the overwhelming majority of cases, the termination is due to the pole angle exceeding the allowed limits - which graphically would result in a series of points exiting the red rectangle at the top or bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48641b0",
   "metadata": {},
   "source": [
    "## Running multiple games\n",
    "\n",
    "In order to see the range of results from an unoptimised process, lets run multiple experiments and see the state when it terminates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exp = 1000\n",
    "score = 0\n",
    "\n",
    "cpos = []\n",
    "pang = []\n",
    "pvel = []\n",
    "score = 0\n",
    "\n",
    "for exp in range(n_exp):\n",
    "    # Set up for loop\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "\n",
    "    # Reset environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    while terminated is False and truncated is False:\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        score+=reward\n",
    "        \n",
    "    cpos.append(observation[0])\n",
    "    pang.append(observation[2])\n",
    "    pvel.append(observation[3])\n",
    "\n",
    "    env.close()\n",
    "\n",
    "x = list(range(n_exp))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(6, 4))\n",
    "fig.suptitle(f\"State of the environment after {n_exp} trials. Average score = {(score/n_exp):.1f}\")\n",
    "\n",
    "axs0 = plt.subplot(221)\n",
    "axs0.hist(cpos, bins=20,range=(-2.4,2.4))\n",
    "axs0.set_title(\"Cart position\")\n",
    "axs0.axvline(x=2.4, linewidth=1, color=\"r\")\n",
    "axs0.axvline(x=-2.4, linewidth=1, color=\"r\")\n",
    "\n",
    "axs1 = plt.subplot(222)\n",
    "axs1.hist(pang,bins=20,range=(-0.3,0.3))\n",
    "axs1.set_title(\"Pole angle\")\n",
    "axs1.axvline(x=0.2095, linewidth=1, color=\"r\")\n",
    "axs1.axvline(x=-0.2095, linewidth=1, color=\"r\")\n",
    "\n",
    "\n",
    "axs2 = plt.subplot(212)\n",
    "axs2.set_title(f\"Pole velocity range: {min(pvel):.1f} to {max(pvel):.1f}\")\n",
    "axs2.scatter(x, pvel, marker=\".\")\n",
    "axs2.set_ylabel(\"Pole velocity\")\n",
    "axs2.set_xlabel(\"Episode\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a3e97",
   "metadata": {},
   "source": [
    "After 1,000 trials it is confirmed that the termination cause for an untrained system is the pole angle exceeding the allowed limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2a771",
   "metadata": {},
   "source": [
    "## Setting up the table\n",
    "\n",
    "First the continuous observations (cart position and velocity, and pole angle and anglular velocity) are binned to allow them to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef165f32",
   "metadata": {},
   "source": [
    "In this example only two features will be used for optimisation. The pole angle - which is the leading cause of experiment termination, and it's angular velocity. The angle data will be binned into 10 and the velocity into 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (10, 10)\n",
    "\n",
    "# Set the upper and lower bounds for the velocity based on the observations of the velocity (±20%)\n",
    "pv = max(abs(min(pvel)), abs(max(pvel)))*1.2\n",
    "lower_bounds = [env.observation_space.low[2], -pv]\n",
    "upper_bounds = [env.observation_space.high[2], pv]\n",
    "\n",
    "def discretizer( _ , __ , angle, pole_velocity ):\n",
    "    \"\"\"\n",
    "    Convert continues state into a discrete state\n",
    "    \n",
    "    From the environment state space, creates equal bins spaning ± pole \n",
    "    angle(+50) radian range\n",
    "    \n",
    "    Argument: \n",
    "        4 observations\n",
    "    Return:\n",
    "        indicies of the bin that the (pole) data would have been placed into\n",
    "    \n",
    "    \"\"\"\n",
    "    estimator = KBinsDiscretizer(n_bins = n_bins,\n",
    "                                 encode='ordinal',\n",
    "                                 strategy='uniform')\n",
    "    \n",
    "    estimator.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,estimator.transform([[angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b74723",
   "metadata": {},
   "source": [
    "Next a table is generated that holds the Q values, with space for each (discrete) state and action combination. After optimisation the values in the table give the expected reward if a particular action is taken while the environment is in that particular state. The table is initalised with zeros, however it doesn't need to be. Purely exploitative functions can be initialised with non-zero values and depending on the type of problem that is being investigated, they can improve overall results as seen with the multi-armed bandit example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2337a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edbaf2",
   "metadata": {},
   "source": [
    "Next a policy is decided. In this case it is the epsilon-greedy policy which will always pick the action that will maximise results. There is no room for exploration with the epsilon-greedy policy so if the initial conditions are such that what is overall a sub-optimal move is the most optimal at that step; the optimisation may not converge to a truly optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa842b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    \"\"\"\n",
    "    Choosing action based on epsilon-greedy policy - always pick \n",
    "    the highest scoring option\n",
    "    \n",
    "    Argument:\n",
    "        indecies of the bin that the (pole) data are in\n",
    "    \n",
    "    Return\n",
    "        Index of the cell containing the highest number \n",
    "        (or the first instance of the number when ties occur )\n",
    "    \"\"\"\n",
    "    return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c39ab4",
   "metadata": {},
   "source": [
    "The Q value needs to be updated as the environment is evolved. Here the reward, and current value of the Q-table in the specific state are used to update the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value(reward, new_state, discount_factor=1 ):\n",
    "    \"\"\"\n",
    "    Temperal diffrence for updating Q-value of state-action pair\n",
    "    \n",
    "    Argument:\n",
    "        Reward for the step\n",
    "        The indicies of the pole data in the Q-table\n",
    "        The discount factor (0 means only consider the immediate reward\n",
    "            1 means fully consider the future optimal rewards)\n",
    "    \n",
    "    Return:\n",
    "        Updated value for the Q-table in the new_state\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    \n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    \n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec15cf9",
   "metadata": {},
   "source": [
    "The learning rate defines how heavily you want to weight particular parts of the optimisation. Ususally the earlier runs are weighted more heavily, so we can learn quicker, before decaying and learning less at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive learning of Learning Rate\n",
    "def learning_rate(n, min_rate=0.01 ):\n",
    "    \"\"\"\n",
    "    Decaying learning rate\n",
    "    Will return 1 for the first 25 points then decay to 0.01\n",
    "    Ensures that the earlier trials contribute more heavily\n",
    "    to the learning\n",
    "    \n",
    "    Argument:\n",
    "        Step of the optimisation\n",
    "        The minimum rate of learning\n",
    "        \n",
    "    Return:\n",
    "        Number between 1 and 0.01\n",
    "    \"\"\"\n",
    "    return max(min_rate, min(1.0, 1.0 - np.log10((n + 1) / 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_nepisodes=250\n",
    "fig = plt.figure(figsize=(6, 2))\n",
    "plt.plot(list(range(example_nepisodes)),list(map(learning_rate, list(range(example_nepisodes)))))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning rate as a function of episode number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ea4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_rate(e, min_rate = 0.1 ):\n",
    "    \"\"\"\n",
    "    Decaying exploration rate\n",
    "    Will return 1 for the first 25 points then decay to 0.1 and\n",
    "    ensure no exploration for the first 25 episodes.\n",
    "    \n",
    "    Argument:\n",
    "        Step of the optimisation\n",
    "        The minimum rate of learning\n",
    "        \n",
    "    Return:\n",
    "        Number between 1 and 0.1\n",
    "        \n",
    "    \"\"\"\n",
    "    return max(min_rate, min(1.0, 1.0 - np.log10((e  + 1) / 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fa537",
   "metadata": {},
   "source": [
    "## Running the trials\n",
    "\n",
    "Now we run through the trials updating the q-tables for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 500\n",
    "print_freq = 150\n",
    "score_card = []\n",
    "for e in range(n_episodes):\n",
    "    score = 0\n",
    "    \n",
    "    # Discretize state into buckets\n",
    "    initial_observation, info = env.reset()\n",
    "    current_state = discretizer(*initial_observation)\n",
    "        \n",
    "    terminated = False; truncated = False\n",
    "    \n",
    "    track_score = True if e%print_freq == 0 else False\n",
    "    \n",
    "    \n",
    "    while terminated is False and truncated is False:\n",
    "        \n",
    "        # policy action \n",
    "        action = policy(current_state) # Apply the epsilon-greedy policy\n",
    "        \n",
    "        # insert random action\n",
    "        if np.random.random() < exploration_rate(e): # exploration rate = 1 for first 25 episodes\n",
    "            action = env.action_space.sample() # sample from the environment action space (0 or 1) \n",
    "         \n",
    "        # increment enviroment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        new_state = discretizer(*obs)\n",
    "        \n",
    "        # Update Q-Table\n",
    "        lr = learning_rate(e)\n",
    "        learnt_value = new_Q_value(reward, new_state)\n",
    "        old_value = Q_table[current_state][action]\n",
    "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "        \n",
    "        current_state = new_state\n",
    "        score += reward\n",
    "            \n",
    "    score_card.append(score)\n",
    "    if track_score:\n",
    "        print(f\"At end of episode {e}, Score = {int(score)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5056796",
   "metadata": {},
   "source": [
    "## Looking at the results\n",
    "\n",
    "As there are only two actions in this example the Q tables can be visualised as heatmaps for each action. The heatmaps show the expected reward from the environment in that state (with pole angle and velocity as specified) if the action was taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e416913",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_qtable = Q_table[:,:,0]\n",
    "right_qtable = Q_table[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae684be",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map={0: \"Left\", 1:\"Right\"}\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "im = ax[0].imshow(left_qtable)\n",
    "im_r = ax[1].imshow(right_qtable)\n",
    "\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax_r = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "plt.colorbar(im, cax=cax)\n",
    "plt.colorbar(im_r, cax=cax_r)\n",
    "\n",
    "for lr in [0,1]:\n",
    "    ax[lr].set_title(f\"Q table for {action_map[lr]}\")\n",
    "    ax[lr].set_ylabel(\"Pole Angle bin\")\n",
    "    ax[lr].set_xlabel(\"Pole Velocity bin\")\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09a8e8",
   "metadata": {},
   "source": [
    "And if we look at the scores after each episode we should see an improvement in the score with increasing episode number. This corresponds to creating an optimal policy. \n",
    "\n",
    "If the optimal policy is found, then the score for later episodes should be 500, corresponding to a truncation-end of the simulation rather than a failure condition being met.\n",
    "\n",
    "If it appears that the system is stabilising around a value lower than 500, this indicates that the state space was not explored fully, and changes in the learning rate, or exploration rate would affect this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3))\n",
    "plt.plot(list(range(n_episodes)),score_card,marker='.',linewidth=0.5)\n",
    "plt.xlim([0,n_episodes])\n",
    "plt.ylim([0,550])\n",
    "plt.hlines(500, 0, n_episodes, colors='r', ls='dashed', lw=0.5)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Score as a function of episode number during Q-learning\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
